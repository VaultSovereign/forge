# Choose ONE provider (set only one of these)
# OLLAMA_HOST=http://localhost:11434
# OPENROUTER_API_KEY=your-openrouter-key
# OPENAI_API_KEY=your-openai-key

# Optional: override the default model for the chosen provider
# MODEL=meta-llama/llama-3.1-70b-instruct

# Compact reality ledger entries (omit artifacts)
# REALITY_LEDGER_COMPACT=1

# Workbench Guardian (BFF) â€” agent entrypoint
# In development, compile agents with `pnpm run build:agents` or let `pnpm run workbench:start`
# do it for you. The default below points to the compiled bundle.
GUARDIAN_AGENT_ENTRY=agents/build/index.js

# Workbench BFF base URL (used by /forge command and Guardian)
# VM_API_BASE=http://localhost:8787

# Google AI Companion proxy URL (Cloud Run proxy used by /companion)
# AI_COMPANION_PROXY_URL=https://<your-cloud-run-url>

# If Cloud Run is private (no unauthenticated access), either:
# - export AI_COMPANION_ID_TOKEN=$(gcloud auth print-identity-token --audiences="$AI_COMPANION_PROXY_URL")
# - or ensure 'gcloud' is installed; Guardian will attempt to mint a token automatically
